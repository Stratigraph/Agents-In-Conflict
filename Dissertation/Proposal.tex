\section{Introduction}\label{introduction}

Political conflicts are complex. This is the expected opening in a dissertation such as this, but it is nevertheless true. They involve cooperation and competition between multiple actors who differ from one another in their capabilities, their objectives, and even their decisionmaking processes. Furthermore, these actors themselves are often complex -- they are generally political entities (primarily states, though increasingly non-state groups as well) which are ultimately composed not only of individuals but of procedures, institutional memory, and other emergent characteristics. To paraphrase an old joke, it's complexity all the way down.

Much of the quantitative study of political science and international relations seeks, naturally enough, to reduce this complexity. Researchers examine specific cases in detail to tease out their dynamics, or analyze large datasets in search of patterns and regularities. These efforts have, by and large, been successful. The early work of L. F. Richardson on the scales of wars \citep{richardson_1948} revealed their power-law distribution, later identified as a signature of complexity \citep{clauset_2009}. More recent efforts include the Political Instability Task Force \citep{goldstone_2005} which has been able to predict the risks of civil conflict and unrest across countries. Game theory has provided a language which can formally describe and analyze strategic interactions. It has yielded useful ideas (such as the concept of a Nash equilibrium, or the prisoners' dilemma), stylized insights, and even forecasting tools \citep{bdm_2010}. Computational methods have generally augmented other methodologies. Machine learning techniques have expanded the tools available for statistical analysis, while agent-based models (ABMs) have yielded rich stylized operationalizations of qualitative theories \citep{cederman_1997,rouleau_2011} as well as some (mostly proprietary) forecasting tools \citep{taylor_2008}.

Event data (standardized, micro-level records of actions by political actors towards one another) has been used in conjunction with all the methodologies described above. The production and analysis of event data has expanded substantially with the growth of available computing power. However, it has not (to the best of my knowledge) been used as a way of directly comparing models and measuring their explanatory and predictive power against empirical data. In this dissertation, I propose to build a framework designed to do just that.

This framework will facilitate the direct comparison of models of political conflict to one another. The majority of both formal and agent-based models implement a particular representation of the world, and then assign a particular decision rule to all the actors within it. I propose to decouple the models of the interaction structure from the actors' decisionmaking model. This will help highlight the assumptions and artifacts embedded in both, and allow me to test different theories by rapidly combining different external and internal model components. Furthermore, the framework will emit standardized event-type data, allowing models to be rapidly compared both to each other and to observed data.

The rest of this proposal is organized as follows. I will describe in more detail the methodologies used in the quantitative study of international conflict, and discuss their strengths and weaknesses. I will then present my proposed methodology, which is intended to bridge some of the gaps identified in the prior literature. I will discuss several models I intend to experiment with, and several datasets I will utilize in my research. Finally, I describe several of the assessment measures I intend use to compare model output with empirical event data.

\section{Current methods in conflict modeling}\label{current-methods-in-conflict-modeling}

\subsection{Statistical models}\label{statistical-models}

Statistical modeling has become the main line of quantitative political science and international relations. In general, researchers will develop a qualitative theory or take one from prior literature, and operationalize it as an econometric model describing the relationship between some dependent variable (e.g.~the outbreak of war) and several independent variables (e.g.~the relative military strength of the belligerents, or a dummy variable for whether they are democracies). The researchers then fit the model to an appropriate dataset. If the model fits the data well (high $R^2$, low mean squared error), and the coefficients on the independent variables are sufficiently large and statistically significant, this is taken as evidence of the theory's validity. A model can be further tested by applying it outside of the sample used to fit it, either by withholding a subset of the data for testing, or (better yet) applying it to future cases outside the dataset altogether.

There are many, many examples of statistical models applied to conflict. There is a reason this has become the most prevalent form of modeling: ``it basically works,'' \citep{schrodt_2004} both as a method of testing theories and for making forecasts. \citet{wayman_1994} review multiple statistical tests of the qualitative predictions of Realist theories, demonstrating that they are insufficient to explain much of the international system. The work of \citet{goldstone_2005} (the PITF) has been shown, using relatively few variables, to predict with a high degree of accuracy which countries will experience internal instability over a two-year period.

While the statistical approach may indeed provide what \citet{schrodt_2004} refers to as `outcome validity', by and large it is not intended to provide `process validity'. The process, by definition, involves the interaction of two or more actors, often over time, whereas statistical methods generally involve a single dependent variable representing some sort of final outcome. By building datasets of many separate cases, the statistical approach can only capture features that they share in common with one another. The PITF work, for example, can predict the broad conditions under which instability is likely to occur, but not the particular actors who will initiate it nor the mechanism by which it will arise. These details vary too much between cases, and thus are less amenable to statistical analysis. The specifics of the instability, however, are of great importance to policymakers seeking to intervene to prevent or mitigate its onset (or, for that matter, to induce it).

Furthermore, the statistical approach also must find ways to reduce the complexity which inherently characterizes the systems in question. To continue with the example of forecasting political instability, one of the variables with the greatest predictive power in those models is infant mortality \citep{goldstone_2005}. Yet high infant mortality is not directly the cause of most instability, nor is it exogenous to the system as a whole \citep{marshall_2008}. Rather, the variable serves as a proxy for a wide variety of other processes and characteristics which are closer to being the `true' drivers of instability.

Finally, applying statistical models to data generated from interactions risks leading to misleading results. \citet{signorino_1999} demonstrates that applying simple logistic regressions to data generated from a stochastic game-theoretic model will lead to systemically incorrect predictions over a wide variety of conditions. Similarly \citet{masad_2014} suggests that applying a naive logistic regression to data generated by a complex, interactive process (in that case, the \citet{schelling_1971} segregation model) can yield statistical models with extremely high fit but which nevertheless differ qualitatively from the true data-generating process.

\subsection{Formal (game-theoretic) models}\label{formal-game-theoretic-models}

Game theory is the mathematical study of interactions (games) between two or more `players' whose payoffs depend not only on their own choices but those of the other players as well. A fundamental assumption of game theory is that the players are rational, and will select the best possible strategy for themselves. Many formal models are not intended to represent specific historically-situated events; they are notional and abstract, meant to capture general characteristics encountered in multiple cases and provide an explanation or prediction of actor behavior across them.

Much of modern game theory was developed explicitly for the study of international conflict, particularly by researchers at the RAND Corporation \citep{gates_1997}. Game-theoretic or formal models are widely used in political science, where they are used to represent conflict \citep{powell_2006}, negotiation \citep{brams_2003}, elections \citep{coughlin_1981} and more. The assumption of rationality is argued to be more applicable to states than it is to individuals, as state decisionmaking involves planning bureaucracies and processes explicitly geared towards reaching the best possible outcome, anticipating other actors' behavior, and reducing the role of emotion or irrationality in the decisionmaking process.

The prevalence of abstract formal models in the political science literature suggests that they can indeed provide valuable insight. Some such models (e.g.~the prisoners' dilemma, or the Hotelling model as applied to positions on an ideological spectrum \citep{stokes_1963}) become part of a broader societal mental model of politics which extends beyond the realm of academic political science. It is less common to see formal models applied to empirical data. \citet{bdm_1988} applies a game-theoretic model to historic conflict data, arguing that the model explains states' decisions whether to escalate conflicts. Similarly, \citet{signorino_1999} proposes a way to use a logistic model to fit actors' utility weights to data from an extensive-form game tree. Using a different (and never fully-specified \citep{scholz_2011}) model, \citet{bdm_1984} attempts to aggregate expert-derived opinions into game-theoretic predictions of actor behavior, an approach which has had mixed success. While \citet{bdm_2010} claims an impressive track record of success, \citet{schrodt_2004} points out that the predictions made in \citet{bdm_1982} did not fare as well.

Many formal models involve only two players, and few decision points. Adding to either rapidly reduces the tractability of the models. Recall that many two-player board games (such as chess and go) are deterministic, with complete information; in theory they are perfectly solvable, but there is no practical way to do so. They involve too many possible moves, and rapidly branch out into intractability. N-player games (where N is finite but greater than 2) become similarly intractable \citep{papadimitriou_2005}. Thus, researchers must limit themselves to simple models which are amenable to analysis, abstracting away much of the complexity which characterizes the empirical world.

Furthermore, it is by no means clear that the assumption of perfect rationality is more appropriate for political decisionmaking than it is at the individual level. Behavioral economics has shown that human players' deviations from perfect rationality are not merely a white-noise error term, but systemically driven by psychological \citep{tversky_1981} and cultural \citep{henrich_2005} factors. Organizations as well are not necessarily perfectly rational, and in fact are characterized by their own systemic biases \citep{shapira_2002}. The research done by the Central Intelligence Agency \citep{heuer_2001}, among others, on the biases that plague intelligence analysis (an important component of states' external decisionmaking process) indicates that even when political actors are actively attempting to rationalize their decisionmaking, they are still far from successful.

\subsection{Computational Methods}\label{computational-methods}

Computational methods are perhaps the most variable, and least frequent, of the approaches described here. I use the phrase very broadly, to mean any method which in practice relies on a computer (or at least numerical computation) to implement, and does not fall into the categories above. In practice, there are roughly three broad subcategories of computational methods: dynamical systems models, machine learning, and agent-based models.

Dynamical systems models represent components of interest as stocks and flows connected to one another and often exhibiting feedback loops \citep{gilbert_2005}. While many such models can ultimately be represented as systems of differential equations, in practice their complexity often requires them to be estimated numerically. Early dynamical systems models relevant to international relations include the \citet{lanchester_1916} models of ancient and modern warfare, and the Richardson arms race model \citep{rapoport_1957}, both of which emerged from the First World War. The introduction of computers, and the growth of interest in systems theory, led to computational systems dynamics models of international relations, such as the Globus Model \citep{bremer_1987}. The Richardson model has been widely tested, and found to be insufficient to explain the dynamics of arms acquisition even between pairs of well-established rivals \citep{dunne_1999}. This approach in general appears to have fallen out of favor.

Machine learning is, essentially, an extension of statistical methods to leverage new computational resources. This is largely how it has been used in political science. \citet{goldstone_2005} experimented with using methods such as neural networks or Random Forests \citep{breiman_2001} before returning to the logistic regression. \citet{schrodt_1997} used a hidden Markov model to predict the dynamics of conflict in the Middle East, and \citet{schrodt_2004} uses several other approaches (decision trees, neural networks and sequence matching) as prediction methods as well. While these approaches allow for more sophisticated functional forms, they suffer from many of the same problems as the more traditional statistical approach, particularly the `process' vs `outcome' validity distinction. Furthermore, these approaches often trade accuracy for interpretability, resulting in models with strong predictive power but which cannot be directly mapped to a qualitative theory.

Finally, and most relevantly for this dissertation, are agent-based models (ABMs). These are models or simulations where each actor is simulated as a discrete agent, and the agents interact with one another within some sort of environment or framework. In general, ABMs are operationalizations of a particular qualitative theory from which the agent behaviors are derived. The agent behavior may consist extremely simple rules such as tit-for-tat \citep{hudson_2004,axelrod_1980}; more complicated heuristics \citep{cederman_1997}; or artificial intelligence-type planning algorithms \citep{taylor_2008}. Rarely do the agents learn or adapt, though there are exceptions \citep{hoffmann_2005,rouleau_2011}. The level of abstraction of ABMs also varies widely. Many are `artificial societies' \citep{epstein_1996} showing how features of the international system can arise from simple qualitative assumptions. \citet{cederman_1997} models an international system as consisting of political actors on a square grid, with actors simply deciding whether to go to war with their neighbors, and what level of forces to allocate to such wars. Meanwhile, \citet{taylor_2008} describe a generic (and non-public) model of political decisionmaking, called the Power Structure Toolkit (PSTK), with agents endowed with detailed beliefs about the world, multiple types and levels of power, and an ability to conduct detailed planning towards their goals. The PSTK provides a toolkit for building scenario-specific models by instantiating agents with capabilities, goals and beliefs provided by subject-matter experts on the specific political system in question. Like in the Cederman model, however, all the agents are ultimately using the same decision rules.

Abstract ABMs serve similar purposes to formal models; however, by relaxing the requirements for perfect rationality and analytic tractability, they can describe richer notional models and explore their properties and emergent behaviors. Such ABMs must qualitatively ground their assumptions, and require additional analysis due to their number of moving parts to ensure that their results robust, and are not merely artifacts of the simulation. Something as simple as the agent activation order may have profound and unanticipated impacts on the model results \citep{comer_2014}. Furthermore, there is not always a clear mapping between the outputs of an abstract ABM and empirical data.

Empirical ABMs appear to be even less common in international relations. Such models often require significant amounts of data, or assumptions when such data is missing \citep{masad_2014B}. Similarly, the more complicated the agent behavior, the more coding is required, adding to the assumptions, parameters to be tested, or potential points of failure. These requirements help suggest why many empirical ABMs of international relations appear to primarily be developed in the commercial or government sectors, including \citet{taylor_2008}, \citet{abdollahian_2006} and \citet{chaturvedi_2000}. Nevertheless, the success of empirical ABMs in other fields such as geography \citep{jantz_2004} and epidemiology \citep{mniszewski_2008} suggests that this approach should be viable.

\subsection{Event Data}\label{event-data}

As the name implies, event data is not strictly a methodology but a type of data which, I will argue, will aid in bridging between several of the methodologies described above. Simply, event data consists of records of the actions taken by various actors, primarily (though not necessarily exclusively) in an international relations context. Events are generally relational, taking the form of \emph{WHO did WHAT to WHOM, WHEN (and sometimes WHERE)}. In other words, an event consists of a source, a target, an action, a timestamp and possibly a location. Event data is generally coded from media sources: originally by hand (primarily by graduate and undergraduate students) and more recently automatically by software.

There are several different event data sets available. Most notably,  the Global Data on Event, Location and Tone (GDELT) \citep{leetaru_2013} gained some prominence in recent years. This dataset, automatically coded from media sources, covered daily events worldwide from 1979 to the present. While the dataset gained substantial media attention, questions arose as to its provenance, and most of the original team withdrew from the project. Previous event data sets include the Kansas Event Data System\citep{schrodt_2006}, like GDELT coded from media sources via the TABARI parser \citep{schrodt_2001a}, and the hand-coded Behavioral Correlates of War (BCOW) dataset \citep{leng_1987} which covers a selected subset of conflicts for the years 1816-1979.

All three methodology classes described above have been applied to event data. For example, \citet{yonamine_2013} used GDELT to fit a statistical model to violent events in Afghanistan; \citet{metternich_2013} compared data from the International Conflict Early Warning System (ICEWS) data (a proprietary Department of Defense event data set) to the predictions of a game-theoretic network model; and \citet{taylor_2009} populate some aspects of the PSTK model from an unnamed event data set, which is likely to be ICEWS as well.

Furthermore, there are multiple models which have not been applied with event data but are nevertheless amenable to it. The Bueno de Mesquita family of models involves actors taking one of a limited set of actions toward one another in discrete time-steps -- in other words, it implicitly generates events. Similarly, \citet{slantchev_2003} describes a formal model of war in which actors advance negotiation positions and military action against each other, again in discrete time-steps, mapping neatly to the `verbal' and `physical' action distinction found in many event data coding schemes.

An important advantage of event data is that it captures not only the ultimate outcome of a political conflict, but its intermediate dynamics as well. Two sets of events may both have the same final high-level outcome (e.g. \emph{Not War}), but the steps by which that outcome was reached may differ substantially. This, in turn, implies that the event sets in question were generated by different underlying processes. This is true of the empirical event data itself, but also of any model that generates such data, either explicitly or implicitly. Thus, we may quantitatively assess models' process validities by comparing the events they generate to empirical data, and to one another. I believe that this is an underutilized feature of event data, and it is this feature which this dissertation will be built around.

Since event data is often derived from media reports, it may not directly represent the `true' events. In fact, it has several well-documented biases, in particular an emphasis on violence and an exhibition of media fatigue \citep{gerner_1998}, and systemic regional differences \citep{schrodt_2001b}. By comparing real event data to the outputs of a plausible, robust model we may be able to identify and quantify some of this bias, and generate hypotheses of missing or overrepresented events in the empirical data.

\section{Research Question}\label{research-question}

The goal of this research is to demonstrate the utility of computational modeling as a tool for bridging between many of the methodologies and tools used in the study of political conflict. At its core, it will attempt to answer the question: \textbf{how can multiple models of interaction and decisionmaking be compared to one another, and to empirical data, in order to evaluate their explanatory and predictive power?}

In order to resolve these questions, I will build a modeling framework capable of implementing many possible models of political interactions, and emitting simulated events from model runs. I will use this framework to replicate existing models and directly compare their outputs to empirical data as well as to each other. I will also develop several additional models, from simple rule-based agents to heuristic-driven planners, and compare their outputs.

\section{Methodology}\label{methodology}

The different paradigms of the quantitative study of international relations (and of conflict and cooperation more broadly) provide a variety of tools for addressing different questions, at the cost of different (and sometimes conflicting) theoretical underpinnings. Furthermore, direct comparison between the results of different methodologies is frequently difficult, making it challenging to evaluate whether one paradigm has greater predictive or explanatory power in a certain context. The purpose of this dissertation is to utilize computer modeling to build a bridge over these gaps, providing a way of directly comparing models and methods within a common context.

\subsection{Framework}\label{framework}

Most agent-based modeling frameworks (e.g.~MASON \citep{luke_2005} and NetLogo \citep{wilensky_1999}) are highly generalized. While these tools are invaluable for creating and analyzing a broad range of models, they provide very little help in comparing models to one another. Two models of the same system, implemented by two different researchers, may have very different structures and outputs. This, in turn, makes it difficult to directly compare the assumptions of the models without deep familiarity with both, or to compare their outputs to each other or to external data. Furthermore, it may be difficult or impossible to `dock' \citep{axtell_1996} two models together -- for example, inserting an agent from one into another, or using one as an input into another.

There also exist several tools for modeling very specific systems, including in the realm of political conflict. Such systems are often proprietary (e.g.~SoarTech's Power Structure Toolkit \citep{taylor_2008} or Bueno De Mesquita's prediction model), and implement fixed and opaque assumptions, behavioral rules, and other features. While extravagant claims are often made as to their validity or power \citep{bdm_2010}, they are difficult to test and directly compare with other models.

The core product of the dissertation will be a new framework meant to fall somewhere between these two extremes. The framework will be designed from the ground up for modeling international conflicts, and political conflicts more broadly. It is intended to allow the implementation of all the paradigms described above, and facilitate their direct comparison.

As \citet{simon_1996} has noted, complex social systems in particular are often characterized by hierarchy. This does not only refer to superior-subordinate relationships, but also to layers of abstraction. This is certainly the case for international systems. Much of international relations theory treats states as `black box' atomic actors, and in practice states often treat one another as such. Indeed, states often have similar options when dealing with one another, especially in cases of conflict: they can use (or threaten to use) military force, apply various economic pressures (e.g.~sanctions), make statements, or cede ground (physically or metaphorically) to another state. Yet in practice, state the mechanisms by which states make decisions vary widely, and may include democratic competition, bureaucratic procedure, or dictatorial whim. Furthermore, states often attempt to assess not only each other's interests, but their decisionmaking process as well.

Agent-based models are uniquely capable of capturing this distinction. However, not many have explored it in great depth. In the \citet{axelrod_1997} model of the emergence of new political actors, agents decide on whether to threaten their neighbors, and how to respond to such threats. While the agents are differentiated by their spatial position and material capabilities, they nevertheless all utilize the same decision rules. The \citet{cederman_1997} model features similar decisions, and uses two (albeit similar) decision rules: one for `predatory' agents and one for the rest. The \citet{axelrod_1980} prisoners' dilemma tournament, however, was anchored on a separation between agents' external interactions and internal decision process. All the agents were playing the same game with one another, either cooperating or defecting; however, each was endowed with a separate decisionmaking process, and it was these processes which were being compared.

The framework I propose is similarly designed from the ground up to separate agents' external attributes and interactions from their internal decision process. The external component of each model defines the environment for political actors, and the set of actions they can take. The actors' decision process is then modeled separately. This allows for several different research possibilities, including the direct comparison of the behavior of different decision models, and easily instantiating populations of agents utilizing heterogeneous decisionmaking rules.

Each model in the framework is structured as follows:

\begin{enumerate} \def\labelenumi{\arabic{enumi}.} \item   \textbf{World:} This defines the `external environment' of the model.   At the most basic level, it must implement the scheduling and   structure for agent interaction and activation: e.g. are agents called to emit   actions into the world one at a time, do they engage in pairwise interactions in   a round-robin format, or do they all act simultaneously? \item   \textbf{Agents:}

  \begin{itemize}   \itemsep1pt\parskip0pt\parsep0pt   \item     Agent structure: This is the `external' aspect of the agent. It     includes fixed methods that each agent should be able to call to     interact with the world, as well as the agent's public information     -- that is, information which is observable by all the other agents.     This definition is essentially part of the world   \item     Agent decisionmaking: This is the `interior' aspect of the agent:     any private information it may have, and especially its decision     rule.   \end{itemize} \item   \textbf{Logger}: Each model also contains a `logger' which emits   records of each agent's actions, and potentially of the state of the   world as well. The use of a standardized logger facilitates the direct   comparison of the models. \end{enumerate}

The framework will also contain tools to build several standard types of worlds and agents. In particular, it will contain a data structure for defining extensive-form games, which as described above are commonly utilized in formal models of crises. Finally, the framework will implement tools for comparing the logged outputs of models to one another, as well as to load the event data sets used as case studies.

This framework will facilitate rapid implementing models of agent interaction and decisionmaking, together or separately. By comparing models operationalizing different theories and paradigms to each other, I will be able to identify and analyze the differences between them, as well as where they resemble one another or make similar predictions. By comparing their output to real data, I will not only be able to identify which model fits the data better, but whether different models fit different portions of the data to different degrees. Such analysis, in turn, can help identify strengths and weaknesses of different qualitative theories, improving our understanding of the theories themselves and of the world, and aid us in generating better predictions.

\subsection{Modeling Paradigms}\label{modeling-paradigms}

\subsubsection{Game Theory}\label{game-theory}

As I describe above, formal game-theoretic models are the main method for explicitly modeling interactions in international relations. The interactions are most often structured as dynamic extensive-form games \citep{signorino_1999,bdm_1992}, or repeating simultaneous-interaction games \citep{slantchev_2003,bdm_2002}, while the agents are assumed to be perfectly rational (and often perfectly informed, or at least perfectly Bayesian-updating \citep{slantchev_2003}) utility maximizers. In some models (e.g. \citet{bdm_2002}) they may be rational utility-maximizers over a fixed time horizon, potentially due to the costs associated with longer-term planning. I have already implemented a decisionmaking module for extensive-form games, capable of finding the subgame-perfect pure strategy equilibrium for games of complete information via backwards induction, bounded by computing resources. I will implement similar utility-maximization decision rules for \citet{bdm_2002} type models, potentially including an ability to look ahead to a variable depth.

\subsubsection{Learning Agents}\label{learning-agents}

As discussed above, the use of machine learning in international relations has largely focused on identifying system-level patterns and predictors. However, as \citet{schrodt_2004} has noted, one characteristic of states is that they learn. This occurs not only via the learning of individuals within the decisionmaking system, but in the development of new institutional structures and procedures, changes in the organizational culture, and other forms of institutional learning.

In fact, the idea that states or organizations learn approximately, by repeating actions which have been effective in the past and avoiding those which have not, has a parallel in reinforcement learning \citep{sutton_1998}. There has been some success in using reinforcement learning in multi-agent contexts \citep{nowe_2012,littman_1994}. However, \citet{galla_2013} also demonstrated that in a wide variety of games, reinforcement learning (RL) agents \emph{do not} converge to an equilibrium strategy. I will explore reinforcement learning as method of modeling political actors' ability to learn from their past experiences, and how that learning affects their future decisions. I will also explore system-level behaviors emerging from the actors' simultaneous learning, and potentially the effects of the presence of non-learning agents in the system.

I have conducted initial work applying RL agents to the extensive-form model of conflict proposed by \citet{signorino_1999}. In initial experiments, pairs of agents learn the equilibrium solution extremely quickly, under a wide variety of parameters. This suggests a wider theoretical conclusion: contrary to the formulations which hold idea of rational actors in contrast to rule-bound bureaucracies (e.g. \citet{allison_1999}, \citet{schrodt_2004}), it may be the case that slow-adapting bureaucracies can nevertheless learn in a reasonable timeframe to behave as though they were rational actors.

I have also conducted initial experiments with random pairings of RL agents from a larger population, where the agents do not utilize any information about their partner in each particular crisis iteration. Despite this, the overall behavior of all the agents appears to grow closer the equilibrium behavior over the course of multiple iterations, though it does not reach it. This suggests that the agents are learning a set of behaviors which are generally advantageous within the simulated international system they find themselves in; and further, that this learning is successful despite the agents' lack of explicit strategic consideration. In game-theoretic terms, we are seeing the emergence of agent `types'. While additional experiments and analysis are needed, the early results suggest that actors may learn to approximate equilibrium play even in a multi-agent system and without explicit strategic consideration.

I will continue with a set of experiments applying reinforcement learning agents to international conflict models (both simulated and using real data), in order to ascertain whether it is a plausible model for state learning, and explore the implications. I will also extend this model by adding case-based reasoning \citep{leake_1994}. This has been used successfully in some other multi-agent models \citep{izquierdo_2004}, and will allow the addition of coarse-grained strategic reasoning to the otherwise non-strategic RL model.

\subsubsection{Heuristic Agents}\label{heuristic-agents}

Heuristic agents are what one generally associates with agent-based modeling; I use the term here as a catch-all for agents which do not fall into either of the above categories. The simplest kind of heuristic agents use a small number of rules. An example of a single simple yet powerful rule is tit-for-tat, where an agent plays the same move that a counterpart played against them in the previous round. In the \citet{axelrod_1980} iterated prisoners' dilemma tournament, this rule emerged as clear winner. \citet{hudson_2004} showed that a similar reciprocal rule could explain a large part of repeated interactions in data on the Israeli-Palestinian conflict. The addition of another simple rule (`Olive Branch' or the occasional unprompted cooperative play) explains still more of the events. Expert systems \citep{taber_1992} are the extreme of rules-driven agent, using expert-derived rules to attempt to directly capture the real actors' decisionmaking process.

Another approach involves mechanisms for agents to look ahead and formulate plans to achieve particular goals. Some of these techniques are anchored in attempts to explicitly model agent cognitions \citep{balke_2014} while others seek plausible and computationally-efficient ways to find satisficing strategies \citep{latek_2009}. There are also techniques drawn from the field of artificial intelligence which allow agents to plan efficiently, though they are not anchored in qualitative human behavior. For example, Monte Carlo Tree Search (MCTS) provides an efficient algorithm for planning in competitive games \citep{chaslot_2008}. While to the best of my knowledge it has not been applied in a computational social science research context, MCTS has been used to provide the strategic AI in commercial political-military video game \citep{champandard_2014}.

I intend to build several simple rule-based agents, and at least one more complicated planning agent. By implementing them within the framework, I will be able to directly compare their dynamics and predictive power to one another, as well as to the other agents.

\subsection{Case Studies}\label{case-studies}

\subsubsection{Crisis Model}\label{crisis-model}

There are several similar variants on the crisis model \citep{signorino_1999,carrubba_2007,bdm_1992}, an extensive-form game describing the process by which a pair of states decide to go to war with one another. One such example is in Figure 1. Despite (or because of) the model's simplicity, it has appeared in several different contexts, with some debate over its interpretation. In particular, \citet{signorino_1999} proposes a simple model of conflict anchored in one variant of the crisis model, and uses it to stochastically generate 1,000 notional conflicts. He then fits several more traditional logistic regressions to them, attempting to demonstrate their inadequacy for explaining data generated by an interactive strategic process. \citet{bdm_1992} argue that a similar model can be used to predict the course of many international crises. As described above, however, these approaches conflate the decisionmaking and interactive aspects of the system: they assume that the crisis game is an adequate model not only of the states' interactions, but of states reasoning about their interactions.

%% SMALL CRISIS USED TO BE EHRE

Following \citet{signorino_1999}, I will use the extensive-form crisis game as a `toy model' to test and demonstrate my approach. I will use my framework to study this model using several different decisionmaking models. As described above, I have already conducted a preliminary replication of the \citet{signorino_1999} stochastic analysis, and conducted initial experiments using reinforcement-learning agents. I will continue to explore several different decision rules. These may include; simple rule-based agents; and agents with reinforcement learning augmented by case-based reasoning \citep{izquierdo_2004}.

By comparing the results of these decision rules with the game-theoretic equilibrium, I will be able to identify the importance of the assumptions which go into them. If the results are similar, it will support the utilization of game-theoretic reasoning in this and similar models; on the other hand, if the results differ in systemic ways, it will suggest particular patterns in real-world data to examine. The most interesting outcome, of course, would be identifying a decisionmaking model which provides a better fit the game-theoretic one. However, even a null result would be valuable as evidence supporting the use of formal models to explain state crisis decisionmaking.

Finally, I will argue that the crisis model emits implicit events at each node of the game tree, as agents take actions escalating or deescalating their conflict. I will be able to further test the model variants by comparing the properties of the events they generate to those observed in reality, using some of the methodologies described below.

\subsubsection{Behavioral Correlates of War}\label{behavioral-correlates-of-war}

The Behavioral Correlates of War (BCOW) \citep{leng_1987} is a dataset covering a selected set of international crises, from 1816 to 1979. For each crisis (some ending in war, others not) the dataset includes hand-coded records of actions taken by the various involved parties; these actions may be physical (e.g.~attacks) or verbal (comments, requests, statements of intentions). In the case of some verbal actions, the event being discussed (e.g.~A requests that B take action X) is also included, up to two layers deep (e.g.~A comments on B's request that C take action Y). The BCOW dataset has been used in several analyses of international conflict. In particular, \citet{schrodt_2004} compares similarity of event sequences between crises and demonstrates that such sequences have predictive power as to their outcome.

I intend to use several different modeling approaches and compare them to the BCOW data. I will implement one or more models where agents interact with each other via actions mapped to categories in the BCOW dataset. I will then implement the corresponding decisionmaking models, using the approaches described above. Finally, I will instantiate simulations of particular conflicts, supplementing BCOW data from other sources, including data on military capabilities and conflict outcomes derived from the Correlates of War project \citep{sarkees_2010}, and if needed conflict-specific qualitative sources.

By comparing the results of these simulated interactions to the observed events, I will be able to assess the model's explanatory and predictive power. By comparing the outputs of multiple models to each other and to the data, I will be able to assess not only whether one model outperforms another, but whether different models have a better or worse fit for different sets of crises, as well as to different phases within each crisis.

I have already conducted initial experiments with applying tit-for-tat agents and and the \citet{scholz_2011} replication of the Bueno De Mesquita decision model to one particular set of BCOW events, the 1957 Honduras-Nicaragua border crisis. While the results are extremely preliminary, they demonstrate that both models can generate event data which can be directly compared to BCOW data. The tit-for-tat model generates many of the observed events, while the BDM model generates none of them but nevertheless converges to a `compromise' outcome comparable to the observed conclusion. These experiments also highlight some of the issues with both types of agents. The initial defection, and later cooperation, experienced by the purely tit-for-tat agents must be set exogenously; meanwhile, the actors and their numeric properties in the BDM-derived model must be specified based on a qualitative and after the fact analysis of the conflict.

\subsubsection{Open Event Data Alliance}\label{open-event-data-alliance}

The Open Event Data Alliance (OEDA) is a group of researchers developing a new, open-sourced event data pipeline \citep{schrodt_2014}. The group arose after the rise of questions surrounding the GDELT dataset \citep{nexon_2014}, and is developing an open alternative for real-time and historic event data generation. Early versions of the pipeline and data are already available, with more expected soon.

GDELT proved particularly useful for studying and forecasting conflicts at the sub-state level \citep{masad_2013,yonamine_2013}. A similar, closed-source dataset, the Integrated Conflict Early Warning System (ICEWS) has also proved effective for studying and modeling non-governmental political actors \citep{metternich_2013}. I anticipate using this data in a similar way, in order to compare and evaluate models of subnational political actors and conflicts.

In addition, the daily updating allows for continuous updating and out-of-sample prediction testing. Using GDELT, \citet{yonamine_2013} built a time-series model of violent events in Afghanistan, fit it iteratively to events up to a certain date, used the fitted model to predict the next period, and compared the results before incorporating the next period into the sample and refitting the model. There are two possible ways I can conduct similar testing. With learning agents, I can attempt to have the agents iteratiely learn from each period's events; if the learning agents' actions (and in particular, any qualitative changes in their behavior) mirror those observed in data, this is evidence both that the real actors are learning, and that the learning method in use is a good model of that learning. Another methodology follows \citet{taylor_2009} and has learning and fitting take place at the \emph{model} level. In this methodology, events are used to derive agents' goals, and populate their beliefs about the world. These data are then incorporated into the agents' planning algorithm, which in turn produces the predicted actions for the next period.

\subsection{Output Analysis}\label{output-analysis}

There are numerous methodologies for comparing model outputs to data. The most commonly used for statistical models are simple visual assessment and standard measures such as $R^2$ and root mean squared error, despite these methods' known problems \citep{schunn_2005}. \citet{khemlani_2014} propose a method for assessing the goodness-of-fit of a model across multiple cases, taking into account not only the accuracy but the number of cases which the model covers. This is useful for studying event-data models across multiple separate intervals or distinct crises. \citet{klemens_2014} lays out a general way of treating an agent-based model like a statistical model for fitting it to data, or evaluating it against data.

Event data is not simply a numeric series, however. Each entry in an event series is not a single number, but rather a collection of events. Additional methodologies have been developed There are several methodologies which have been developed to study event data time series, which I will apply to analyze the output.

\subsubsection{Numeric Aggregation}\label{numeric-aggregation}

The simplest methodology, and thus one of the most widely-used, is to convert the event data series into a numeric time series. For example, the Goldstein Scale \citep{goldstein_1992} maps each event type to a number between -10 (Military attack) and 8.3 (Extend military assistance). All the events in a given time period (e.g.~in a day, or a month) can then be mapped to numbers, which in turn can be summed, averaged, or otherwise aggregated. The utility of this methodology is that it is simple, and allows us to bring standard statistical tools to bear. We can examine trends in the aggregated Goldstein score, observe the correlation between two event series, and so forth. The weakness of this methodology, however, is the loss of information it entails. For example, we may not want to not be able to distinguish between the pair of events `military attack' and `explain or state policy' (Goldstein scores -10 and 0, respectively) and a single event `order person or personnel out of country' (Goldstein score -5). Is a military attack really \emph{twice as bad} as an expulsion, in the example above?

\subsubsection{Event Distributions}\label{event-distributions}

All event types do not occur with equal probability or frequency. Comments and statements are far more common than military attacks, which in turn are more common than the the use of nuclear weapons. In fact, \citet{schrodt_2008} demonstrates that event frequencies seem to follow a power law distribution. Comparing the distribution of events being generated by a model to those observed in reality may provide an effective way of assessing the model. Even in the absence of a close match in time between observed and notional events, if a model generates a plausible distribution of events it may be evidence of its broader plausibility. This is particularly important if we believe the observed events themselves to be unlikely; if this is indeed the case, a plausible model of reality will be similarly unlikely to match the observed events.

\subsubsection{Sequence Matching}\label{sequence-matching}

This approach involves comparing the similarity or distance between a pair of event sets or sequences. There are numerous methods of estimating this distance, including Jaccard Similarity \citep{tan_2005} which measures how many events which occur in one set or the other occur in both; and Levenshtein Distance \citep{levenshtein_1966}, which measures how many events must be changed to make one sequence match another. Sequence matching is described in \citet{schrodt_2004} as a method of prediction: identifying past sequences which are similar to a target sequence, and predicting that the next events in the target sequence will likely follow those in the past. However, a similar approach could be used to measure how similar a simulated event sequence is to the sequence observed in reality.

\subsubsection{Network Analysis}\label{network-analysis}

One of the key questions in international relations is how states and other actors decide to form alliances. Alliances, and cooperative (and competitive) relationships more broadly, can be conceived as networks \citep{maoz_2010}. The actors are nodes, linked by edges which may be directed (e.g.~sending of military assistance) or undirected (e.g.~a mutual-defense treaty). Any conflict model involving more than two actors is, implicitly, a network-formation model as well, as actors decide who to assist or oppose. Thus, I can analyze the results of such models using network-analysis techniques to compare the simulated networks to those observed in reality. Furthermore, if a model covers a sufficient timeframe for the network to change or evolve, the dynamics of the simulated network can be compared to the observed dynamics as well.

These comparisons may be a direct measure of similarity (e.g. the Jaccard similarity between the edges in the simulated and observed networks), which can serve as a test of the model's predictive power. We may also compare various network metrics, such as degree distribution. As discussed with event distribution above, a model generating networks with plausible degree distribution or other structural features may be successsfully capturing some of the process and generating plausible outcomes even if it is not predicting the particular outcome observed in reality.

The current state-of-the-art in political network formation modeling is exponential random graph modeling (ERGMs) \citep{robins_2007} which estimates the importance of different properties (at the network, edge and node level) for network formation. These provide another methodology against which models can be tested. If an agent-based model does not provide more explanatory power than an ERGM, its marginal utility may be limited, at least as a model of network formation. On the other hand, if an agent-based model provides superior results, that is strong evidence of its usefulness.